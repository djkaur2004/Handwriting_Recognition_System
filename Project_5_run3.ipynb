{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8407c156-39cc-48fb-8d14-e4d4e0f6b061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Importing Modules\n",
    "# ============================================\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# make results dir\n",
    "os.makedirs(\"results\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b946ed-ac32-411e-97af-6377a8c5fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Dataset Loading (fixed char maps + robust label parsing)\n",
    "# ============================================\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, labels_file, images_dir, transform=None, characters=None):\n",
    "        \"\"\"\n",
    "        labels_file: each line -> \"filename label\" (label may contain spaces)\n",
    "        images_dir: directory containing images referenced in labels_file\n",
    "        transform: torchvision transforms to apply\n",
    "        characters: optional string of valid characters (keeps mapping stable)\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        if characters is None:\n",
    "            # you can customize which characters you want to support\n",
    "            characters = string.ascii_letters + string.digits + string.punctuation + \" \"\n",
    "        self.characters = characters\n",
    "\n",
    "        # Mapping: reserve 0 for CTC blank / padding\n",
    "        self.char_to_num = {c: i + 1 for i, c in enumerate(self.characters)}\n",
    "        self.num_to_char = {i + 1: c for i, c in enumerate(self.characters)}\n",
    "        self.num_to_char[0] = \"\"  # blank / padding\n",
    "\n",
    "        if not os.path.exists(labels_file):\n",
    "            raise FileNotFoundError(f\"Labels file not found: {labels_file}\")\n",
    "\n",
    "        with open(labels_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                if line == \"\":\n",
    "                    continue\n",
    "                parts = line.strip().split(maxsplit=1)\n",
    "                if len(parts) == 0:\n",
    "                    continue\n",
    "                if len(parts) == 1:\n",
    "                    filename = parts[0]\n",
    "                    label = \"\"\n",
    "                else:\n",
    "                    filename, label = parts\n",
    "                img_path = os.path.join(images_dir, filename)\n",
    "                if os.path.exists(img_path) and label.strip() != \"\":\n",
    "                    self.samples.append((img_path, label))\n",
    "                # else: skip missing files or empty labels\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        # unknown characters map to 0 (blank/padding) â€” but better to remove unknown chars\n",
    "        return torch.tensor([self.char_to_num.get(c, 0) for c in text], dtype=torch.long)\n",
    "\n",
    "    def decode_indices(self, indices):\n",
    "        # indices: sequence of ints\n",
    "        return \"\".join([self.num_to_char.get(int(i), \"\") for i in indices if i != 0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"L\")  # grayscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label_encoded = self.encode_text(label)\n",
    "        return img, label_encoded, label  # also return original label string for convenience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82e7209-8f6c-4f74-9c85-d60fbbe06371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader and Val loader created. num_workers=0, pin_memory=False\n",
      "Train batches: 1250, Val batches: 250\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. Preprocessing of Dataset + collate_fn (CTC target handling fixed)\n",
    "# ============================================\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Data augmentation and transforms (stronger)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.RandomAffine(degrees=3, translate=(0.02,0.02), shear=2, scale=(0.95,1.05), fill=255),\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# character set â€” keep it stable and reasonably small if you have limited labels\n",
    "characters = string.ascii_letters + string.digits + string.punctuation + \" \"\n",
    "\n",
    "train_dataset = OCRDataset(\n",
    "    labels_file=r\"C:\\Users\\acer\\OneDrive\\Desktop\\Handwriting_recognition\\data_split_reduced\\train\\labels.txt\",\n",
    "    images_dir=r\"C:\\Users\\acer\\OneDrive\\Desktop\\Handwriting_recognition\\data_split_reduced\\train\\images\",\n",
    "    transform=train_transform,\n",
    "    characters=characters\n",
    ")\n",
    "\n",
    "val_dataset = OCRDataset(\n",
    "    labels_file=r\"C:\\Users\\acer\\OneDrive\\Desktop\\Handwriting_recognition\\data_split_reduced\\val\\labels.txt\",\n",
    "    images_dir=r\"C:\\Users\\acer\\OneDrive\\Desktop\\Handwriting_recognition\\data_split_reduced\\val\\images\",\n",
    "    transform=val_transform,\n",
    "    characters=characters\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      imgs: tensor (B, 1, H, W)\n",
    "      targets: 1D tensor concatenating all label indices (for CTC)\n",
    "      target_lengths: tensor (B,) lengths of each label\n",
    "      labels_padded: padded labels (B, max_len) -- useful for reconstructing true_texts\n",
    "      raw_texts: list of original label strings\n",
    "    \"\"\"\n",
    "    imgs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    raw_texts = [item[2] for item in batch]\n",
    "\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "\n",
    "    # create padded labels for reconstruction/printing\n",
    "    if len(labels) == 0:\n",
    "        return imgs, torch.tensor([], dtype=torch.long), label_lengths, torch.tensor([], dtype=torch.long), raw_texts\n",
    "\n",
    "    labels_padded = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)  # (B, Lmax)\n",
    "\n",
    "    # flatten targets for CTCLoss: 1D concatenation of all labels (no padding)\n",
    "    targets = torch.cat([l for l in labels]).to(torch.long)  # 1D\n",
    "\n",
    "    return imgs, targets, label_lengths, labels_padded, raw_texts\n",
    "\n",
    "# ============================================\n",
    "# 3b. DataLoader creation (DEBUG-FRIENDLY / Windows-safe)\n",
    "# Replace your previous train_loader / val_loader definitions with these.\n",
    "# ============================================\n",
    "# Note: use num_workers=0 on Windows or with OneDrive paths to avoid hangs.\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,        # <-- IMPORTANT: 0 avoids worker deadlocks on Windows / cloud drives\n",
    "    pin_memory=False,     # safer when num_workers=0 and avoids some platform issues\n",
    "    persistent_workers=False,\n",
    "    timeout=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,        # <-- keep 0 for safety\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    timeout=0\n",
    ")\n",
    "\n",
    "print(\"Train loader and Val loader created. num_workers=0, pin_memory=False\")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b486ae-9e5d-4d4d-838f-07494450dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "  )\n",
      "  (rnn): LSTM(1024, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=96, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. Define Model (Improved CRNN + CTC)\n",
    "# ============================================\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, img_h, num_classes, dropout=0.2):\n",
    "        super(CRNN, self).__init__()\n",
    "        # conv blocks\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2),  # H/2\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2),  # H/4\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            # keep width resolution; no further pooling in height to preserve sequence length\n",
    "        )\n",
    "        self.img_h = img_h\n",
    "        # RNN: input size = (channels_after_cnn * (H//4))\n",
    "        rnn_input_size = (img_h // 4) * 128\n",
    "        self.rnn = nn.LSTM(input_size=rnn_input_size, hidden_size=256, num_layers=2, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(256*2, num_classes + 1)  # +1 for CTC blank\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, H, W)\n",
    "        x = self.cnn(x)  # (B, C, H', W')\n",
    "        b, c, h, w = x.size()\n",
    "        # collapse height and channels into features per time-step (width dimension is time)\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, W', C, H')\n",
    "        x = x.contiguous().view(b, w, c * h)  # (B, W', C*H')\n",
    "        x, _ = self.rnn(x)  # (B, W', 2*hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)  # (B, W', num_classes+1)\n",
    "        x = x.log_softmax(2)  # for CTC Loss\n",
    "        return x\n",
    "\n",
    "num_classes = len(characters)\n",
    "model = CRNN(IMG_HEIGHT, num_classes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1da6d60-d51d-4ba5-884c-782afcf1cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 5.6343\n",
      "Batch 100/1250 - Avg Train Loss: 4.7587\n",
      "Batch 150/1250 - Avg Train Loss: 4.3796\n",
      "Batch 200/1250 - Avg Train Loss: 4.1355\n",
      "Batch 250/1250 - Avg Train Loss: 3.9871\n",
      "Batch 300/1250 - Avg Train Loss: 3.8637\n",
      "Batch 350/1250 - Avg Train Loss: 3.7585\n",
      "Batch 400/1250 - Avg Train Loss: 3.6963\n",
      "Batch 450/1250 - Avg Train Loss: 3.6282\n",
      "Batch 500/1250 - Avg Train Loss: 3.5785\n",
      "Batch 550/1250 - Avg Train Loss: 3.5245\n",
      "Batch 600/1250 - Avg Train Loss: 3.4788\n",
      "Batch 650/1250 - Avg Train Loss: 3.4338\n",
      "Batch 700/1250 - Avg Train Loss: 3.4018\n",
      "Batch 750/1250 - Avg Train Loss: 3.3740\n",
      "Batch 800/1250 - Avg Train Loss: 3.3439\n",
      "Batch 850/1250 - Avg Train Loss: 3.3168\n",
      "Batch 900/1250 - Avg Train Loss: 3.2968\n",
      "Batch 950/1250 - Avg Train Loss: 3.2728\n",
      "Batch 1000/1250 - Avg Train Loss: 3.2502\n",
      "Batch 1050/1250 - Avg Train Loss: 3.2240\n",
      "Batch 1100/1250 - Avg Train Loss: 3.2021\n",
      "Batch 1150/1250 - Avg Train Loss: 3.1802\n",
      "Batch 1200/1250 - Avg Train Loss: 3.1595\n",
      "Batch 1250/1250 - Avg Train Loss: 3.1383\n",
      "Epoch [1/30] Train Loss: 3.138264 | Val Loss: 2.551070 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 2/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 2.6105\n",
      "Batch 100/1250 - Avg Train Loss: 2.5989\n",
      "Batch 150/1250 - Avg Train Loss: 2.6191\n",
      "Batch 200/1250 - Avg Train Loss: 2.5954\n",
      "Batch 250/1250 - Avg Train Loss: 2.5977\n",
      "Batch 300/1250 - Avg Train Loss: 2.5980\n",
      "Batch 350/1250 - Avg Train Loss: 2.5722\n",
      "Batch 400/1250 - Avg Train Loss: 2.5706\n",
      "Batch 450/1250 - Avg Train Loss: 2.5659\n",
      "Batch 500/1250 - Avg Train Loss: 2.5656\n",
      "Batch 550/1250 - Avg Train Loss: 2.5528\n",
      "Batch 600/1250 - Avg Train Loss: 2.5469\n",
      "Batch 650/1250 - Avg Train Loss: 2.5423\n",
      "Batch 700/1250 - Avg Train Loss: 2.5305\n",
      "Batch 750/1250 - Avg Train Loss: 2.5172\n",
      "Batch 800/1250 - Avg Train Loss: 2.5143\n",
      "Batch 850/1250 - Avg Train Loss: 2.5055\n",
      "Batch 900/1250 - Avg Train Loss: 2.5054\n",
      "Batch 950/1250 - Avg Train Loss: 2.4989\n",
      "Batch 1000/1250 - Avg Train Loss: 2.4933\n",
      "Batch 1050/1250 - Avg Train Loss: 2.4799\n",
      "Batch 1100/1250 - Avg Train Loss: 2.4724\n",
      "Batch 1150/1250 - Avg Train Loss: 2.4688\n",
      "Batch 1200/1250 - Avg Train Loss: 2.4631\n",
      "Batch 1250/1250 - Avg Train Loss: 2.4580\n",
      "Epoch [2/30] Train Loss: 2.457962 | Val Loss: 2.201204 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 3/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 2.1458\n",
      "Batch 100/1250 - Avg Train Loss: 2.1818\n",
      "Batch 150/1250 - Avg Train Loss: 2.2068\n",
      "Batch 200/1250 - Avg Train Loss: 2.2129\n",
      "Batch 250/1250 - Avg Train Loss: 2.2248\n",
      "Batch 300/1250 - Avg Train Loss: 2.2203\n",
      "Batch 350/1250 - Avg Train Loss: 2.2111\n",
      "Batch 400/1250 - Avg Train Loss: 2.2184\n",
      "Batch 450/1250 - Avg Train Loss: 2.2157\n",
      "Batch 500/1250 - Avg Train Loss: 2.2012\n",
      "Batch 550/1250 - Avg Train Loss: 2.2038\n",
      "Batch 600/1250 - Avg Train Loss: 2.1839\n",
      "Batch 650/1250 - Avg Train Loss: 2.1817\n",
      "Batch 700/1250 - Avg Train Loss: 2.1760\n",
      "Batch 750/1250 - Avg Train Loss: 2.1688\n",
      "Batch 800/1250 - Avg Train Loss: 2.1575\n",
      "Batch 850/1250 - Avg Train Loss: 2.1519\n",
      "Batch 900/1250 - Avg Train Loss: 2.1504\n",
      "Batch 950/1250 - Avg Train Loss: 2.1461\n",
      "Batch 1000/1250 - Avg Train Loss: 2.1455\n",
      "Batch 1050/1250 - Avg Train Loss: 2.1488\n",
      "Batch 1100/1250 - Avg Train Loss: 2.1476\n",
      "Batch 1150/1250 - Avg Train Loss: 2.1405\n",
      "Batch 1200/1250 - Avg Train Loss: 2.1337\n",
      "Batch 1250/1250 - Avg Train Loss: 2.1283\n",
      "Epoch [3/30] Train Loss: 2.128314 | Val Loss: 1.898639 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 4/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 1.9993\n",
      "Batch 100/1250 - Avg Train Loss: 1.8959\n",
      "Batch 150/1250 - Avg Train Loss: 1.9060\n",
      "Batch 200/1250 - Avg Train Loss: 1.9228\n",
      "Batch 250/1250 - Avg Train Loss: 1.8961\n",
      "Batch 300/1250 - Avg Train Loss: 1.8965\n",
      "Batch 350/1250 - Avg Train Loss: 1.8951\n",
      "Batch 400/1250 - Avg Train Loss: 1.8908\n",
      "Batch 450/1250 - Avg Train Loss: 1.8922\n",
      "Batch 500/1250 - Avg Train Loss: 1.8879\n",
      "Batch 550/1250 - Avg Train Loss: 1.9014\n",
      "Batch 600/1250 - Avg Train Loss: 1.9038\n",
      "Batch 650/1250 - Avg Train Loss: 1.8954\n",
      "Batch 700/1250 - Avg Train Loss: 1.8927\n",
      "Batch 750/1250 - Avg Train Loss: 1.8904\n",
      "Batch 800/1250 - Avg Train Loss: 1.8902\n",
      "Batch 850/1250 - Avg Train Loss: 1.8893\n",
      "Batch 900/1250 - Avg Train Loss: 1.8841\n",
      "Batch 950/1250 - Avg Train Loss: 1.8793\n",
      "Batch 1000/1250 - Avg Train Loss: 1.8840\n",
      "Batch 1050/1250 - Avg Train Loss: 1.8779\n",
      "Batch 1100/1250 - Avg Train Loss: 1.8696\n",
      "Batch 1150/1250 - Avg Train Loss: 1.8669\n",
      "Batch 1200/1250 - Avg Train Loss: 1.8623\n",
      "Batch 1250/1250 - Avg Train Loss: 1.8574\n",
      "Epoch [4/30] Train Loss: 1.857446 | Val Loss: 1.651359 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 5/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 1.7341\n",
      "Batch 100/1250 - Avg Train Loss: 1.7377\n",
      "Batch 150/1250 - Avg Train Loss: 1.7215\n",
      "Batch 200/1250 - Avg Train Loss: 1.7301\n",
      "Batch 250/1250 - Avg Train Loss: 1.7063\n",
      "Batch 300/1250 - Avg Train Loss: 1.7027\n",
      "Batch 350/1250 - Avg Train Loss: 1.6907\n",
      "Batch 400/1250 - Avg Train Loss: 1.6832\n",
      "Batch 450/1250 - Avg Train Loss: 1.6879\n",
      "Batch 500/1250 - Avg Train Loss: 1.6949\n",
      "Batch 550/1250 - Avg Train Loss: 1.6921\n",
      "Batch 600/1250 - Avg Train Loss: 1.6935\n",
      "Batch 650/1250 - Avg Train Loss: 1.6862\n",
      "Batch 700/1250 - Avg Train Loss: 1.6787\n",
      "Batch 750/1250 - Avg Train Loss: 1.6745\n",
      "Batch 800/1250 - Avg Train Loss: 1.6651\n",
      "Batch 850/1250 - Avg Train Loss: 1.6558\n",
      "Batch 900/1250 - Avg Train Loss: 1.6482\n",
      "Batch 950/1250 - Avg Train Loss: 1.6452\n",
      "Batch 1000/1250 - Avg Train Loss: 1.6406\n",
      "Batch 1050/1250 - Avg Train Loss: 1.6367\n",
      "Batch 1100/1250 - Avg Train Loss: 1.6315\n",
      "Batch 1150/1250 - Avg Train Loss: 1.6255\n",
      "Batch 1200/1250 - Avg Train Loss: 1.6192\n",
      "Batch 1250/1250 - Avg Train Loss: 1.6173\n",
      "Epoch [5/30] Train Loss: 1.617302 | Val Loss: 1.464139 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 6/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 1.4579\n",
      "Batch 100/1250 - Avg Train Loss: 1.4456\n",
      "Batch 150/1250 - Avg Train Loss: 1.4301\n",
      "Batch 200/1250 - Avg Train Loss: 1.4221\n",
      "Batch 250/1250 - Avg Train Loss: 1.4169\n",
      "Batch 300/1250 - Avg Train Loss: 1.4116\n",
      "Batch 350/1250 - Avg Train Loss: 1.4089\n",
      "Batch 400/1250 - Avg Train Loss: 1.3968\n",
      "Batch 450/1250 - Avg Train Loss: 1.3980\n",
      "Batch 500/1250 - Avg Train Loss: 1.3944\n",
      "Batch 550/1250 - Avg Train Loss: 1.3879\n",
      "Batch 600/1250 - Avg Train Loss: 1.3872\n",
      "Batch 650/1250 - Avg Train Loss: 1.3862\n",
      "Batch 700/1250 - Avg Train Loss: 1.3896\n",
      "Batch 750/1250 - Avg Train Loss: 1.3907\n",
      "Batch 800/1250 - Avg Train Loss: 1.3990\n",
      "Batch 850/1250 - Avg Train Loss: 1.3951\n",
      "Batch 900/1250 - Avg Train Loss: 1.3860\n",
      "Batch 950/1250 - Avg Train Loss: 1.3879\n",
      "Batch 1000/1250 - Avg Train Loss: 1.3880\n",
      "Batch 1050/1250 - Avg Train Loss: 1.3882\n",
      "Batch 1100/1250 - Avg Train Loss: 1.3893\n",
      "Batch 1150/1250 - Avg Train Loss: 1.3838\n",
      "Batch 1200/1250 - Avg Train Loss: 1.3821\n",
      "Batch 1250/1250 - Avg Train Loss: 1.3796\n",
      "Epoch [6/30] Train Loss: 1.379628 | Val Loss: 1.200211 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 7/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 1.1962\n",
      "Batch 100/1250 - Avg Train Loss: 1.2706\n",
      "Batch 150/1250 - Avg Train Loss: 1.2418\n",
      "Batch 200/1250 - Avg Train Loss: 1.2570\n",
      "Batch 250/1250 - Avg Train Loss: 1.2612\n",
      "Batch 300/1250 - Avg Train Loss: 1.2554\n",
      "Batch 350/1250 - Avg Train Loss: 1.2498\n",
      "Batch 400/1250 - Avg Train Loss: 1.2460\n",
      "Batch 450/1250 - Avg Train Loss: 1.2419\n",
      "Batch 500/1250 - Avg Train Loss: 1.2412\n",
      "Batch 550/1250 - Avg Train Loss: 1.2302\n",
      "Batch 600/1250 - Avg Train Loss: 1.2254\n",
      "Batch 650/1250 - Avg Train Loss: 1.2213\n",
      "Batch 700/1250 - Avg Train Loss: 1.2160\n",
      "Batch 750/1250 - Avg Train Loss: 1.2107\n",
      "Batch 800/1250 - Avg Train Loss: 1.2116\n",
      "Batch 850/1250 - Avg Train Loss: 1.2079\n",
      "Batch 900/1250 - Avg Train Loss: 1.2018\n",
      "Batch 950/1250 - Avg Train Loss: 1.1974\n",
      "Batch 1000/1250 - Avg Train Loss: 1.1949\n",
      "Batch 1050/1250 - Avg Train Loss: 1.1922\n",
      "Batch 1100/1250 - Avg Train Loss: 1.1918\n",
      "Batch 1150/1250 - Avg Train Loss: 1.1903\n",
      "Batch 1200/1250 - Avg Train Loss: 1.1889\n",
      "Batch 1250/1250 - Avg Train Loss: 1.1884\n",
      "Epoch [7/30] Train Loss: 1.188425 | Val Loss: 1.077758 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 8/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 1.0167\n",
      "Batch 100/1250 - Avg Train Loss: 1.0124\n",
      "Batch 150/1250 - Avg Train Loss: 1.0018\n",
      "Batch 200/1250 - Avg Train Loss: 1.0119\n",
      "Batch 250/1250 - Avg Train Loss: 1.0256\n",
      "Batch 300/1250 - Avg Train Loss: 1.0246\n",
      "Batch 350/1250 - Avg Train Loss: 1.0270\n",
      "Batch 400/1250 - Avg Train Loss: 1.0408\n",
      "Batch 450/1250 - Avg Train Loss: 1.0342\n",
      "Batch 500/1250 - Avg Train Loss: 1.0252\n",
      "Batch 550/1250 - Avg Train Loss: 1.0294\n",
      "Batch 600/1250 - Avg Train Loss: 1.0280\n",
      "Batch 650/1250 - Avg Train Loss: 1.0243\n",
      "Batch 700/1250 - Avg Train Loss: 1.0255\n",
      "Batch 750/1250 - Avg Train Loss: 1.0271\n",
      "Batch 800/1250 - Avg Train Loss: 1.0296\n",
      "Batch 850/1250 - Avg Train Loss: 1.0326\n",
      "Batch 900/1250 - Avg Train Loss: 1.0295\n",
      "Batch 950/1250 - Avg Train Loss: 1.0324\n",
      "Batch 1000/1250 - Avg Train Loss: 1.0352\n",
      "Batch 1050/1250 - Avg Train Loss: 1.0383\n",
      "Batch 1100/1250 - Avg Train Loss: 1.0376\n",
      "Batch 1150/1250 - Avg Train Loss: 1.0412\n",
      "Batch 1200/1250 - Avg Train Loss: 1.0447\n",
      "Batch 1250/1250 - Avg Train Loss: 1.0461\n",
      "Epoch [8/30] Train Loss: 1.046103 | Val Loss: 1.044728 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 9/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.9255\n",
      "Batch 100/1250 - Avg Train Loss: 0.9566\n",
      "Batch 150/1250 - Avg Train Loss: 0.9727\n",
      "Batch 200/1250 - Avg Train Loss: 0.9923\n",
      "Batch 250/1250 - Avg Train Loss: 0.9725\n",
      "Batch 300/1250 - Avg Train Loss: 0.9638\n",
      "Batch 350/1250 - Avg Train Loss: 0.9593\n",
      "Batch 400/1250 - Avg Train Loss: 0.9588\n",
      "Batch 450/1250 - Avg Train Loss: 0.9549\n",
      "Batch 500/1250 - Avg Train Loss: 0.9469\n",
      "Batch 550/1250 - Avg Train Loss: 0.9430\n",
      "Batch 600/1250 - Avg Train Loss: 0.9395\n",
      "Batch 650/1250 - Avg Train Loss: 0.9462\n",
      "Batch 700/1250 - Avg Train Loss: 0.9474\n",
      "Batch 750/1250 - Avg Train Loss: 0.9475\n",
      "Batch 800/1250 - Avg Train Loss: 0.9426\n",
      "Batch 850/1250 - Avg Train Loss: 0.9380\n",
      "Batch 900/1250 - Avg Train Loss: 0.9394\n",
      "Batch 950/1250 - Avg Train Loss: 0.9372\n",
      "Batch 1000/1250 - Avg Train Loss: 0.9423\n",
      "Batch 1050/1250 - Avg Train Loss: 0.9404\n",
      "Batch 1100/1250 - Avg Train Loss: 0.9409\n",
      "Batch 1150/1250 - Avg Train Loss: 0.9370\n",
      "Batch 1200/1250 - Avg Train Loss: 0.9364\n",
      "Batch 1250/1250 - Avg Train Loss: 0.9378\n",
      "Epoch [9/30] Train Loss: 0.937788 | Val Loss: 0.920408 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 10/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.7542\n",
      "Batch 100/1250 - Avg Train Loss: 0.7833\n",
      "Batch 150/1250 - Avg Train Loss: 0.8274\n",
      "Batch 200/1250 - Avg Train Loss: 0.8457\n",
      "Batch 250/1250 - Avg Train Loss: 0.8596\n",
      "Batch 300/1250 - Avg Train Loss: 0.8573\n",
      "Batch 350/1250 - Avg Train Loss: 0.8456\n",
      "Batch 400/1250 - Avg Train Loss: 0.8469\n",
      "Batch 450/1250 - Avg Train Loss: 0.8395\n",
      "Batch 500/1250 - Avg Train Loss: 0.8326\n",
      "Batch 550/1250 - Avg Train Loss: 0.8326\n",
      "Batch 600/1250 - Avg Train Loss: 0.8374\n",
      "Batch 650/1250 - Avg Train Loss: 0.8356\n",
      "Batch 700/1250 - Avg Train Loss: 0.8374\n",
      "Batch 750/1250 - Avg Train Loss: 0.8363\n",
      "Batch 800/1250 - Avg Train Loss: 0.8394\n",
      "Batch 850/1250 - Avg Train Loss: 0.8380\n",
      "Batch 900/1250 - Avg Train Loss: 0.8367\n",
      "Batch 950/1250 - Avg Train Loss: 0.8406\n",
      "Batch 1000/1250 - Avg Train Loss: 0.8419\n",
      "Batch 1050/1250 - Avg Train Loss: 0.8448\n",
      "Batch 1100/1250 - Avg Train Loss: 0.8448\n",
      "Batch 1150/1250 - Avg Train Loss: 0.8418\n",
      "Batch 1200/1250 - Avg Train Loss: 0.8395\n",
      "Batch 1250/1250 - Avg Train Loss: 0.8375\n",
      "Epoch [10/30] Train Loss: 0.837503 | Val Loss: 0.863546 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 11/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.7200\n",
      "Batch 100/1250 - Avg Train Loss: 0.6929\n",
      "Batch 150/1250 - Avg Train Loss: 0.7216\n",
      "Batch 200/1250 - Avg Train Loss: 0.7371\n",
      "Batch 250/1250 - Avg Train Loss: 0.7403\n",
      "Batch 300/1250 - Avg Train Loss: 0.7527\n",
      "Batch 350/1250 - Avg Train Loss: 0.7608\n",
      "Batch 400/1250 - Avg Train Loss: 0.7591\n",
      "Batch 450/1250 - Avg Train Loss: 0.7540\n",
      "Batch 500/1250 - Avg Train Loss: 0.7522\n",
      "Batch 550/1250 - Avg Train Loss: 0.7579\n",
      "Batch 600/1250 - Avg Train Loss: 0.7559\n",
      "Batch 650/1250 - Avg Train Loss: 0.7536\n",
      "Batch 700/1250 - Avg Train Loss: 0.7550\n",
      "Batch 750/1250 - Avg Train Loss: 0.7576\n",
      "Batch 800/1250 - Avg Train Loss: 0.7580\n",
      "Batch 850/1250 - Avg Train Loss: 0.7593\n",
      "Batch 900/1250 - Avg Train Loss: 0.7566\n",
      "Batch 950/1250 - Avg Train Loss: 0.7567\n",
      "Batch 1000/1250 - Avg Train Loss: 0.7583\n",
      "Batch 1050/1250 - Avg Train Loss: 0.7593\n",
      "Batch 1100/1250 - Avg Train Loss: 0.7603\n",
      "Batch 1150/1250 - Avg Train Loss: 0.7589\n",
      "Batch 1200/1250 - Avg Train Loss: 0.7582\n",
      "Batch 1250/1250 - Avg Train Loss: 0.7610\n",
      "Epoch [11/30] Train Loss: 0.761003 | Val Loss: 0.855087 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 12/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.6560\n",
      "Batch 100/1250 - Avg Train Loss: 0.6609\n",
      "Batch 150/1250 - Avg Train Loss: 0.6585\n",
      "Batch 200/1250 - Avg Train Loss: 0.6607\n",
      "Batch 250/1250 - Avg Train Loss: 0.6761\n",
      "Batch 300/1250 - Avg Train Loss: 0.6803\n",
      "Batch 350/1250 - Avg Train Loss: 0.6824\n",
      "Batch 400/1250 - Avg Train Loss: 0.6817\n",
      "Batch 450/1250 - Avg Train Loss: 0.6812\n",
      "Batch 500/1250 - Avg Train Loss: 0.6829\n",
      "Batch 550/1250 - Avg Train Loss: 0.6761\n",
      "Batch 600/1250 - Avg Train Loss: 0.6732\n",
      "Batch 650/1250 - Avg Train Loss: 0.6796\n",
      "Batch 700/1250 - Avg Train Loss: 0.6793\n",
      "Batch 750/1250 - Avg Train Loss: 0.6813\n",
      "Batch 800/1250 - Avg Train Loss: 0.6832\n",
      "Batch 850/1250 - Avg Train Loss: 0.6848\n",
      "Batch 900/1250 - Avg Train Loss: 0.6847\n",
      "Batch 950/1250 - Avg Train Loss: 0.6839\n",
      "Batch 1000/1250 - Avg Train Loss: 0.6835\n",
      "Batch 1050/1250 - Avg Train Loss: 0.6827\n",
      "Batch 1100/1250 - Avg Train Loss: 0.6835\n",
      "Batch 1150/1250 - Avg Train Loss: 0.6890\n",
      "Batch 1200/1250 - Avg Train Loss: 0.6931\n",
      "Batch 1250/1250 - Avg Train Loss: 0.6992\n",
      "Epoch [12/30] Train Loss: 0.699228 | Val Loss: 0.806973 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 13/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.6196\n",
      "Batch 100/1250 - Avg Train Loss: 0.6314\n",
      "Batch 150/1250 - Avg Train Loss: 0.6257\n",
      "Batch 200/1250 - Avg Train Loss: 0.6198\n",
      "Batch 250/1250 - Avg Train Loss: 0.6152\n",
      "Batch 300/1250 - Avg Train Loss: 0.6044\n",
      "Batch 350/1250 - Avg Train Loss: 0.6122\n",
      "Batch 400/1250 - Avg Train Loss: 0.6198\n",
      "Batch 450/1250 - Avg Train Loss: 0.6176\n",
      "Batch 500/1250 - Avg Train Loss: 0.6136\n",
      "Batch 550/1250 - Avg Train Loss: 0.6151\n",
      "Batch 600/1250 - Avg Train Loss: 0.6150\n",
      "Batch 650/1250 - Avg Train Loss: 0.6140\n",
      "Batch 700/1250 - Avg Train Loss: 0.6177\n",
      "Batch 750/1250 - Avg Train Loss: 0.6212\n",
      "Batch 800/1250 - Avg Train Loss: 0.6265\n",
      "Batch 850/1250 - Avg Train Loss: 0.6284\n",
      "Batch 900/1250 - Avg Train Loss: 0.6309\n",
      "Batch 950/1250 - Avg Train Loss: 0.6326\n",
      "Batch 1000/1250 - Avg Train Loss: 0.6343\n",
      "Batch 1050/1250 - Avg Train Loss: 0.6342\n",
      "Batch 1100/1250 - Avg Train Loss: 0.6340\n",
      "Batch 1150/1250 - Avg Train Loss: 0.6355\n",
      "Batch 1200/1250 - Avg Train Loss: 0.6383\n",
      "Batch 1250/1250 - Avg Train Loss: 0.6377\n",
      "Epoch [13/30] Train Loss: 0.637729 | Val Loss: 0.771820 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 14/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.5101\n",
      "Batch 100/1250 - Avg Train Loss: 0.5345\n",
      "Batch 150/1250 - Avg Train Loss: 0.5368\n",
      "Batch 200/1250 - Avg Train Loss: 0.5534\n",
      "Batch 250/1250 - Avg Train Loss: 0.5738\n",
      "Batch 300/1250 - Avg Train Loss: 0.5706\n",
      "Batch 350/1250 - Avg Train Loss: 0.5811\n",
      "Batch 400/1250 - Avg Train Loss: 0.5876\n",
      "Batch 450/1250 - Avg Train Loss: 0.5835\n",
      "Batch 500/1250 - Avg Train Loss: 0.5891\n",
      "Batch 550/1250 - Avg Train Loss: 0.5900\n",
      "Batch 600/1250 - Avg Train Loss: 0.5933\n",
      "Batch 650/1250 - Avg Train Loss: 0.5962\n",
      "Batch 700/1250 - Avg Train Loss: 0.5940\n",
      "Batch 750/1250 - Avg Train Loss: 0.5913\n",
      "Batch 800/1250 - Avg Train Loss: 0.5954\n",
      "Batch 850/1250 - Avg Train Loss: 0.5962\n",
      "Batch 900/1250 - Avg Train Loss: 0.5925\n",
      "Batch 950/1250 - Avg Train Loss: 0.5967\n",
      "Batch 1000/1250 - Avg Train Loss: 0.5980\n",
      "Batch 1050/1250 - Avg Train Loss: 0.5997\n",
      "Batch 1100/1250 - Avg Train Loss: 0.6000\n",
      "Batch 1150/1250 - Avg Train Loss: 0.6004\n",
      "Batch 1200/1250 - Avg Train Loss: 0.6028\n",
      "Batch 1250/1250 - Avg Train Loss: 0.5999\n",
      "Epoch [14/30] Train Loss: 0.599880 | Val Loss: 0.772673 | LR: 0.001000\n",
      "\n",
      "===== Epoch 15/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.5616\n",
      "Batch 100/1250 - Avg Train Loss: 0.5581\n",
      "Batch 150/1250 - Avg Train Loss: 0.5297\n",
      "Batch 200/1250 - Avg Train Loss: 0.5120\n",
      "Batch 250/1250 - Avg Train Loss: 0.5004\n",
      "Batch 300/1250 - Avg Train Loss: 0.4970\n",
      "Batch 350/1250 - Avg Train Loss: 0.4958\n",
      "Batch 400/1250 - Avg Train Loss: 0.5004\n",
      "Batch 450/1250 - Avg Train Loss: 0.5056\n",
      "Batch 500/1250 - Avg Train Loss: 0.5129\n",
      "Batch 550/1250 - Avg Train Loss: 0.5161\n",
      "Batch 600/1250 - Avg Train Loss: 0.5205\n",
      "Batch 650/1250 - Avg Train Loss: 0.5277\n",
      "Batch 700/1250 - Avg Train Loss: 0.5378\n",
      "Batch 750/1250 - Avg Train Loss: 0.5377\n",
      "Batch 800/1250 - Avg Train Loss: 0.5361\n",
      "Batch 850/1250 - Avg Train Loss: 0.5371\n",
      "Batch 900/1250 - Avg Train Loss: 0.5417\n",
      "Batch 950/1250 - Avg Train Loss: 0.5406\n",
      "Batch 1000/1250 - Avg Train Loss: 0.5396\n",
      "Batch 1050/1250 - Avg Train Loss: 0.5428\n",
      "Batch 1100/1250 - Avg Train Loss: 0.5469\n",
      "Batch 1150/1250 - Avg Train Loss: 0.5473\n",
      "Batch 1200/1250 - Avg Train Loss: 0.5484\n",
      "Batch 1250/1250 - Avg Train Loss: 0.5473\n",
      "Epoch [15/30] Train Loss: 0.547261 | Val Loss: 0.731772 | LR: 0.001000\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 16/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.5097\n",
      "Batch 100/1250 - Avg Train Loss: 0.5218\n",
      "Batch 150/1250 - Avg Train Loss: 0.4850\n",
      "Batch 200/1250 - Avg Train Loss: 0.4913\n",
      "Batch 250/1250 - Avg Train Loss: 0.4829\n",
      "Batch 300/1250 - Avg Train Loss: 0.4820\n",
      "Batch 350/1250 - Avg Train Loss: 0.4789\n",
      "Batch 400/1250 - Avg Train Loss: 0.4803\n",
      "Batch 450/1250 - Avg Train Loss: 0.4850\n",
      "Batch 500/1250 - Avg Train Loss: 0.4861\n",
      "Batch 550/1250 - Avg Train Loss: 0.4943\n",
      "Batch 600/1250 - Avg Train Loss: 0.4961\n",
      "Batch 650/1250 - Avg Train Loss: 0.4987\n",
      "Batch 700/1250 - Avg Train Loss: 0.5015\n",
      "Batch 750/1250 - Avg Train Loss: 0.5019\n",
      "Batch 800/1250 - Avg Train Loss: 0.5019\n",
      "Batch 850/1250 - Avg Train Loss: 0.5049\n",
      "Batch 900/1250 - Avg Train Loss: 0.5049\n",
      "Batch 950/1250 - Avg Train Loss: 0.5064\n",
      "Batch 1000/1250 - Avg Train Loss: 0.5074\n",
      "Batch 1050/1250 - Avg Train Loss: 0.5068\n",
      "Batch 1100/1250 - Avg Train Loss: 0.5111\n",
      "Batch 1150/1250 - Avg Train Loss: 0.5131\n",
      "Batch 1200/1250 - Avg Train Loss: 0.5144\n",
      "Batch 1250/1250 - Avg Train Loss: 0.5158\n",
      "Epoch [16/30] Train Loss: 0.515763 | Val Loss: 0.777372 | LR: 0.001000\n",
      "\n",
      "===== Epoch 17/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.4300\n",
      "Batch 100/1250 - Avg Train Loss: 0.4632\n",
      "Batch 150/1250 - Avg Train Loss: 0.4593\n",
      "Batch 200/1250 - Avg Train Loss: 0.4526\n",
      "Batch 250/1250 - Avg Train Loss: 0.4563\n",
      "Batch 300/1250 - Avg Train Loss: 0.4522\n",
      "Batch 350/1250 - Avg Train Loss: 0.4562\n",
      "Batch 400/1250 - Avg Train Loss: 0.4594\n",
      "Batch 450/1250 - Avg Train Loss: 0.4615\n",
      "Batch 500/1250 - Avg Train Loss: 0.4659\n",
      "Batch 550/1250 - Avg Train Loss: 0.4701\n",
      "Batch 600/1250 - Avg Train Loss: 0.4727\n",
      "Batch 650/1250 - Avg Train Loss: 0.4707\n",
      "Batch 700/1250 - Avg Train Loss: 0.4694\n",
      "Batch 750/1250 - Avg Train Loss: 0.4738\n",
      "Batch 800/1250 - Avg Train Loss: 0.4748\n",
      "Batch 850/1250 - Avg Train Loss: 0.4737\n",
      "Batch 900/1250 - Avg Train Loss: 0.4741\n",
      "Batch 950/1250 - Avg Train Loss: 0.4764\n",
      "Batch 1000/1250 - Avg Train Loss: 0.4767\n",
      "Batch 1050/1250 - Avg Train Loss: 0.4737\n",
      "Batch 1100/1250 - Avg Train Loss: 0.4759\n",
      "Batch 1150/1250 - Avg Train Loss: 0.4771\n",
      "Batch 1200/1250 - Avg Train Loss: 0.4789\n",
      "Batch 1250/1250 - Avg Train Loss: 0.4802\n",
      "Epoch [17/30] Train Loss: 0.480173 | Val Loss: 0.736923 | LR: 0.001000\n",
      "\n",
      "===== Epoch 18/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.5088\n",
      "Batch 100/1250 - Avg Train Loss: 0.4656\n",
      "Batch 150/1250 - Avg Train Loss: 0.4470\n",
      "Batch 200/1250 - Avg Train Loss: 0.4413\n",
      "Batch 250/1250 - Avg Train Loss: 0.4364\n",
      "Batch 300/1250 - Avg Train Loss: 0.4271\n",
      "Batch 350/1250 - Avg Train Loss: 0.4371\n",
      "Batch 400/1250 - Avg Train Loss: 0.4416\n",
      "Batch 450/1250 - Avg Train Loss: 0.4432\n",
      "Batch 500/1250 - Avg Train Loss: 0.4406\n",
      "Batch 550/1250 - Avg Train Loss: 0.4405\n",
      "Batch 600/1250 - Avg Train Loss: 0.4422\n",
      "Batch 650/1250 - Avg Train Loss: 0.4430\n",
      "Batch 700/1250 - Avg Train Loss: 0.4435\n",
      "Batch 750/1250 - Avg Train Loss: 0.4442\n",
      "Batch 800/1250 - Avg Train Loss: 0.4442\n",
      "Batch 850/1250 - Avg Train Loss: 0.4458\n",
      "Batch 900/1250 - Avg Train Loss: 0.4474\n",
      "Batch 950/1250 - Avg Train Loss: 0.4510\n",
      "Batch 1000/1250 - Avg Train Loss: 0.4498\n",
      "Batch 1050/1250 - Avg Train Loss: 0.4499\n",
      "Batch 1100/1250 - Avg Train Loss: 0.4480\n",
      "Batch 1150/1250 - Avg Train Loss: 0.4498\n",
      "Batch 1200/1250 - Avg Train Loss: 0.4534\n",
      "Batch 1250/1250 - Avg Train Loss: 0.4546\n",
      "Epoch [18/30] Train Loss: 0.454636 | Val Loss: 0.738763 | LR: 0.001000\n",
      "\n",
      "===== Epoch 19/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.4175\n",
      "Batch 100/1250 - Avg Train Loss: 0.4263\n",
      "Batch 150/1250 - Avg Train Loss: 0.4131\n",
      "Batch 200/1250 - Avg Train Loss: 0.4080\n",
      "Batch 250/1250 - Avg Train Loss: 0.4206\n",
      "Batch 300/1250 - Avg Train Loss: 0.4200\n",
      "Batch 350/1250 - Avg Train Loss: 0.4124\n",
      "Batch 400/1250 - Avg Train Loss: 0.4094\n",
      "Batch 450/1250 - Avg Train Loss: 0.4088\n",
      "Batch 500/1250 - Avg Train Loss: 0.4122\n",
      "Batch 550/1250 - Avg Train Loss: 0.4095\n",
      "Batch 600/1250 - Avg Train Loss: 0.4044\n",
      "Batch 650/1250 - Avg Train Loss: 0.4079\n",
      "Batch 700/1250 - Avg Train Loss: 0.4073\n",
      "Batch 750/1250 - Avg Train Loss: 0.4053\n",
      "Batch 800/1250 - Avg Train Loss: 0.4056\n",
      "Batch 850/1250 - Avg Train Loss: 0.4080\n",
      "Batch 900/1250 - Avg Train Loss: 0.4091\n",
      "Batch 950/1250 - Avg Train Loss: 0.4100\n",
      "Batch 1000/1250 - Avg Train Loss: 0.4122\n",
      "Batch 1050/1250 - Avg Train Loss: 0.4165\n",
      "Batch 1100/1250 - Avg Train Loss: 0.4209\n",
      "Batch 1150/1250 - Avg Train Loss: 0.4237\n",
      "Batch 1200/1250 - Avg Train Loss: 0.4244\n",
      "Batch 1250/1250 - Avg Train Loss: 0.4268\n",
      "ðŸ”» LR reduced from 0.001000 to 0.000500\n",
      "Epoch [19/30] Train Loss: 0.426771 | Val Loss: 0.734925 | LR: 0.000500\n",
      "\n",
      "===== Epoch 20/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.3455\n",
      "Batch 100/1250 - Avg Train Loss: 0.3475\n",
      "Batch 150/1250 - Avg Train Loss: 0.3384\n",
      "Batch 200/1250 - Avg Train Loss: 0.3385\n",
      "Batch 250/1250 - Avg Train Loss: 0.3398\n",
      "Batch 300/1250 - Avg Train Loss: 0.3375\n",
      "Batch 350/1250 - Avg Train Loss: 0.3339\n",
      "Batch 400/1250 - Avg Train Loss: 0.3316\n",
      "Batch 450/1250 - Avg Train Loss: 0.3356\n",
      "Batch 500/1250 - Avg Train Loss: 0.3351\n",
      "Batch 550/1250 - Avg Train Loss: 0.3336\n",
      "Batch 600/1250 - Avg Train Loss: 0.3332\n",
      "Batch 650/1250 - Avg Train Loss: 0.3340\n",
      "Batch 700/1250 - Avg Train Loss: 0.3315\n",
      "Batch 750/1250 - Avg Train Loss: 0.3303\n",
      "Batch 800/1250 - Avg Train Loss: 0.3302\n",
      "Batch 850/1250 - Avg Train Loss: 0.3282\n",
      "Batch 900/1250 - Avg Train Loss: 0.3273\n",
      "Batch 950/1250 - Avg Train Loss: 0.3261\n",
      "Batch 1000/1250 - Avg Train Loss: 0.3248\n",
      "Batch 1050/1250 - Avg Train Loss: 0.3256\n",
      "Batch 1100/1250 - Avg Train Loss: 0.3245\n",
      "Batch 1150/1250 - Avg Train Loss: 0.3268\n",
      "Batch 1200/1250 - Avg Train Loss: 0.3264\n",
      "Batch 1250/1250 - Avg Train Loss: 0.3235\n",
      "Epoch [20/30] Train Loss: 0.323456 | Val Loss: 0.651848 | LR: 0.000500\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 21/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.2750\n",
      "Batch 100/1250 - Avg Train Loss: 0.2787\n",
      "Batch 150/1250 - Avg Train Loss: 0.2751\n",
      "Batch 200/1250 - Avg Train Loss: 0.2822\n",
      "Batch 250/1250 - Avg Train Loss: 0.2842\n",
      "Batch 300/1250 - Avg Train Loss: 0.2855\n",
      "Batch 350/1250 - Avg Train Loss: 0.2828\n",
      "Batch 400/1250 - Avg Train Loss: 0.2838\n",
      "Batch 450/1250 - Avg Train Loss: 0.2839\n",
      "Batch 500/1250 - Avg Train Loss: 0.2822\n",
      "Batch 550/1250 - Avg Train Loss: 0.2792\n",
      "Batch 600/1250 - Avg Train Loss: 0.2786\n",
      "Batch 650/1250 - Avg Train Loss: 0.2821\n",
      "Batch 700/1250 - Avg Train Loss: 0.2824\n",
      "Batch 750/1250 - Avg Train Loss: 0.2819\n",
      "Batch 800/1250 - Avg Train Loss: 0.2829\n",
      "Batch 850/1250 - Avg Train Loss: 0.2846\n",
      "Batch 900/1250 - Avg Train Loss: 0.2843\n",
      "Batch 950/1250 - Avg Train Loss: 0.2867\n",
      "Batch 1000/1250 - Avg Train Loss: 0.2872\n",
      "Batch 1050/1250 - Avg Train Loss: 0.2856\n",
      "Batch 1100/1250 - Avg Train Loss: 0.2843\n",
      "Batch 1150/1250 - Avg Train Loss: 0.2847\n",
      "Batch 1200/1250 - Avg Train Loss: 0.2847\n",
      "Batch 1250/1250 - Avg Train Loss: 0.2861\n",
      "Epoch [21/30] Train Loss: 0.286087 | Val Loss: 0.650329 | LR: 0.000500\n",
      "ðŸ’¾ Saved best model to results/best_ocr_model.pth\n",
      "\n",
      "===== Epoch 22/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.2829\n",
      "Batch 100/1250 - Avg Train Loss: 0.2599\n",
      "Batch 150/1250 - Avg Train Loss: 0.2686\n",
      "Batch 200/1250 - Avg Train Loss: 0.2718\n",
      "Batch 250/1250 - Avg Train Loss: 0.2665\n",
      "Batch 300/1250 - Avg Train Loss: 0.2677\n",
      "Batch 350/1250 - Avg Train Loss: 0.2681\n",
      "Batch 400/1250 - Avg Train Loss: 0.2669\n",
      "Batch 450/1250 - Avg Train Loss: 0.2704\n",
      "Batch 500/1250 - Avg Train Loss: 0.2694\n",
      "Batch 550/1250 - Avg Train Loss: 0.2747\n",
      "Batch 600/1250 - Avg Train Loss: 0.2718\n",
      "Batch 650/1250 - Avg Train Loss: 0.2723\n",
      "Batch 700/1250 - Avg Train Loss: 0.2724\n",
      "Batch 750/1250 - Avg Train Loss: 0.2724\n",
      "Batch 800/1250 - Avg Train Loss: 0.2753\n",
      "Batch 850/1250 - Avg Train Loss: 0.2734\n",
      "Batch 900/1250 - Avg Train Loss: 0.2743\n",
      "Batch 950/1250 - Avg Train Loss: 0.2739\n",
      "Batch 1000/1250 - Avg Train Loss: 0.2747\n",
      "Batch 1050/1250 - Avg Train Loss: 0.2732\n",
      "Batch 1100/1250 - Avg Train Loss: 0.2726\n",
      "Batch 1150/1250 - Avg Train Loss: 0.2725\n",
      "Batch 1200/1250 - Avg Train Loss: 0.2706\n",
      "Batch 1250/1250 - Avg Train Loss: 0.2700\n",
      "Epoch [22/30] Train Loss: 0.270016 | Val Loss: 0.684193 | LR: 0.000500\n",
      "\n",
      "===== Epoch 23/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.2334\n",
      "Batch 100/1250 - Avg Train Loss: 0.2292\n",
      "Batch 150/1250 - Avg Train Loss: 0.2413\n",
      "Batch 200/1250 - Avg Train Loss: 0.2431\n",
      "Batch 250/1250 - Avg Train Loss: 0.2476\n",
      "Batch 300/1250 - Avg Train Loss: 0.2510\n",
      "Batch 350/1250 - Avg Train Loss: 0.2492\n",
      "Batch 400/1250 - Avg Train Loss: 0.2465\n",
      "Batch 450/1250 - Avg Train Loss: 0.2471\n",
      "Batch 500/1250 - Avg Train Loss: 0.2479\n",
      "Batch 550/1250 - Avg Train Loss: 0.2491\n",
      "Batch 600/1250 - Avg Train Loss: 0.2507\n",
      "Batch 650/1250 - Avg Train Loss: 0.2510\n",
      "Batch 700/1250 - Avg Train Loss: 0.2517\n",
      "Batch 750/1250 - Avg Train Loss: 0.2504\n",
      "Batch 800/1250 - Avg Train Loss: 0.2509\n",
      "Batch 850/1250 - Avg Train Loss: 0.2503\n",
      "Batch 900/1250 - Avg Train Loss: 0.2505\n",
      "Batch 950/1250 - Avg Train Loss: 0.2496\n",
      "Batch 1000/1250 - Avg Train Loss: 0.2494\n",
      "Batch 1050/1250 - Avg Train Loss: 0.2507\n",
      "Batch 1100/1250 - Avg Train Loss: 0.2510\n",
      "Batch 1150/1250 - Avg Train Loss: 0.2513\n",
      "Batch 1200/1250 - Avg Train Loss: 0.2512\n",
      "Batch 1250/1250 - Avg Train Loss: 0.2513\n",
      "Epoch [23/30] Train Loss: 0.251288 | Val Loss: 0.693079 | LR: 0.000500\n",
      "\n",
      "===== Epoch 24/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.1724\n",
      "Batch 100/1250 - Avg Train Loss: 0.1825\n",
      "Batch 150/1250 - Avg Train Loss: 0.1922\n",
      "Batch 200/1250 - Avg Train Loss: 0.2057\n",
      "Batch 250/1250 - Avg Train Loss: 0.2121\n",
      "Batch 300/1250 - Avg Train Loss: 0.2129\n",
      "Batch 350/1250 - Avg Train Loss: 0.2162\n",
      "Batch 400/1250 - Avg Train Loss: 0.2149\n",
      "Batch 450/1250 - Avg Train Loss: 0.2166\n",
      "Batch 500/1250 - Avg Train Loss: 0.2188\n",
      "Batch 550/1250 - Avg Train Loss: 0.2201\n",
      "Batch 600/1250 - Avg Train Loss: 0.2243\n",
      "Batch 650/1250 - Avg Train Loss: 0.2244\n",
      "Batch 700/1250 - Avg Train Loss: 0.2274\n",
      "Batch 750/1250 - Avg Train Loss: 0.2264\n",
      "Batch 800/1250 - Avg Train Loss: 0.2258\n",
      "Batch 850/1250 - Avg Train Loss: 0.2242\n",
      "Batch 900/1250 - Avg Train Loss: 0.2249\n",
      "Batch 950/1250 - Avg Train Loss: 0.2257\n",
      "Batch 1000/1250 - Avg Train Loss: 0.2269\n",
      "Batch 1050/1250 - Avg Train Loss: 0.2276\n",
      "Batch 1100/1250 - Avg Train Loss: 0.2292\n",
      "Batch 1150/1250 - Avg Train Loss: 0.2300\n",
      "Batch 1200/1250 - Avg Train Loss: 0.2312\n",
      "Batch 1250/1250 - Avg Train Loss: 0.2333\n",
      "Epoch [24/30] Train Loss: 0.233342 | Val Loss: 0.706272 | LR: 0.000500\n",
      "\n",
      "===== Epoch 25/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.2288\n",
      "Batch 100/1250 - Avg Train Loss: 0.2351\n",
      "Batch 150/1250 - Avg Train Loss: 0.2207\n",
      "Batch 200/1250 - Avg Train Loss: 0.2294\n",
      "Batch 250/1250 - Avg Train Loss: 0.2241\n",
      "Batch 300/1250 - Avg Train Loss: 0.2283\n",
      "Batch 350/1250 - Avg Train Loss: 0.2315\n",
      "Batch 400/1250 - Avg Train Loss: 0.2266\n",
      "Batch 450/1250 - Avg Train Loss: 0.2309\n",
      "Batch 500/1250 - Avg Train Loss: 0.2318\n",
      "Batch 550/1250 - Avg Train Loss: 0.2320\n",
      "Batch 600/1250 - Avg Train Loss: 0.2320\n",
      "Batch 650/1250 - Avg Train Loss: 0.2313\n",
      "Batch 700/1250 - Avg Train Loss: 0.2302\n",
      "Batch 750/1250 - Avg Train Loss: 0.2314\n",
      "Batch 800/1250 - Avg Train Loss: 0.2320\n",
      "Batch 850/1250 - Avg Train Loss: 0.2296\n",
      "Batch 900/1250 - Avg Train Loss: 0.2289\n",
      "Batch 950/1250 - Avg Train Loss: 0.2274\n",
      "Batch 1000/1250 - Avg Train Loss: 0.2271\n",
      "Batch 1050/1250 - Avg Train Loss: 0.2282\n",
      "Batch 1100/1250 - Avg Train Loss: 0.2307\n",
      "Batch 1150/1250 - Avg Train Loss: 0.2300\n",
      "Batch 1200/1250 - Avg Train Loss: 0.2299\n",
      "Batch 1250/1250 - Avg Train Loss: 0.2292\n",
      "ðŸ”» LR reduced from 0.000500 to 0.000250\n",
      "Epoch [25/30] Train Loss: 0.229235 | Val Loss: 0.677394 | LR: 0.000250\n",
      "\n",
      "===== Epoch 26/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.1723\n",
      "Batch 100/1250 - Avg Train Loss: 0.1753\n",
      "Batch 150/1250 - Avg Train Loss: 0.1899\n",
      "Batch 200/1250 - Avg Train Loss: 0.1911\n",
      "Batch 250/1250 - Avg Train Loss: 0.1938\n",
      "Batch 300/1250 - Avg Train Loss: 0.1938\n",
      "Batch 350/1250 - Avg Train Loss: 0.1973\n",
      "Batch 400/1250 - Avg Train Loss: 0.1975\n",
      "Batch 450/1250 - Avg Train Loss: 0.1959\n",
      "Batch 500/1250 - Avg Train Loss: 0.1980\n",
      "Batch 550/1250 - Avg Train Loss: 0.1949\n",
      "Batch 600/1250 - Avg Train Loss: 0.1939\n",
      "Batch 650/1250 - Avg Train Loss: 0.1912\n",
      "Batch 700/1250 - Avg Train Loss: 0.1899\n",
      "Batch 750/1250 - Avg Train Loss: 0.1892\n",
      "Batch 800/1250 - Avg Train Loss: 0.1889\n",
      "Batch 850/1250 - Avg Train Loss: 0.1886\n",
      "Batch 900/1250 - Avg Train Loss: 0.1880\n",
      "Batch 950/1250 - Avg Train Loss: 0.1863\n",
      "Batch 1000/1250 - Avg Train Loss: 0.1854\n",
      "Batch 1050/1250 - Avg Train Loss: 0.1843\n",
      "Batch 1100/1250 - Avg Train Loss: 0.1830\n",
      "Batch 1150/1250 - Avg Train Loss: 0.1828\n",
      "Batch 1200/1250 - Avg Train Loss: 0.1835\n",
      "Batch 1250/1250 - Avg Train Loss: 0.1843\n",
      "Epoch [26/30] Train Loss: 0.184273 | Val Loss: 0.659293 | LR: 0.000250\n",
      "\n",
      "===== Epoch 27/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.1785\n",
      "Batch 100/1250 - Avg Train Loss: 0.1666\n",
      "Batch 150/1250 - Avg Train Loss: 0.1711\n",
      "Batch 200/1250 - Avg Train Loss: 0.1618\n",
      "Batch 250/1250 - Avg Train Loss: 0.1608\n",
      "Batch 300/1250 - Avg Train Loss: 0.1618\n",
      "Batch 350/1250 - Avg Train Loss: 0.1610\n",
      "Batch 400/1250 - Avg Train Loss: 0.1656\n",
      "Batch 450/1250 - Avg Train Loss: 0.1659\n",
      "Batch 500/1250 - Avg Train Loss: 0.1647\n",
      "Batch 550/1250 - Avg Train Loss: 0.1665\n",
      "Batch 600/1250 - Avg Train Loss: 0.1675\n",
      "Batch 650/1250 - Avg Train Loss: 0.1702\n",
      "Batch 700/1250 - Avg Train Loss: 0.1677\n",
      "Batch 750/1250 - Avg Train Loss: 0.1658\n",
      "Batch 800/1250 - Avg Train Loss: 0.1665\n",
      "Batch 850/1250 - Avg Train Loss: 0.1665\n",
      "Batch 900/1250 - Avg Train Loss: 0.1682\n",
      "Batch 950/1250 - Avg Train Loss: 0.1686\n",
      "Batch 1000/1250 - Avg Train Loss: 0.1687\n",
      "Batch 1050/1250 - Avg Train Loss: 0.1692\n",
      "Batch 1100/1250 - Avg Train Loss: 0.1701\n",
      "Batch 1150/1250 - Avg Train Loss: 0.1706\n",
      "Batch 1200/1250 - Avg Train Loss: 0.1724\n",
      "Batch 1250/1250 - Avg Train Loss: 0.1728\n",
      "Epoch [27/30] Train Loss: 0.172755 | Val Loss: 0.651768 | LR: 0.000250\n",
      "\n",
      "===== Epoch 28/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.1888\n",
      "Batch 100/1250 - Avg Train Loss: 0.1648\n",
      "Batch 150/1250 - Avg Train Loss: 0.1638\n",
      "Batch 200/1250 - Avg Train Loss: 0.1609\n",
      "Batch 250/1250 - Avg Train Loss: 0.1621\n",
      "Batch 300/1250 - Avg Train Loss: 0.1569\n",
      "Batch 350/1250 - Avg Train Loss: 0.1578\n",
      "Batch 400/1250 - Avg Train Loss: 0.1613\n",
      "Batch 450/1250 - Avg Train Loss: 0.1578\n",
      "Batch 500/1250 - Avg Train Loss: 0.1569\n",
      "Batch 550/1250 - Avg Train Loss: 0.1571\n",
      "Batch 600/1250 - Avg Train Loss: 0.1576\n",
      "Batch 650/1250 - Avg Train Loss: 0.1578\n",
      "Batch 700/1250 - Avg Train Loss: 0.1576\n",
      "Batch 750/1250 - Avg Train Loss: 0.1572\n",
      "Batch 800/1250 - Avg Train Loss: 0.1573\n",
      "Batch 850/1250 - Avg Train Loss: 0.1574\n",
      "Batch 900/1250 - Avg Train Loss: 0.1573\n",
      "Batch 950/1250 - Avg Train Loss: 0.1578\n",
      "Batch 1000/1250 - Avg Train Loss: 0.1577\n",
      "Batch 1050/1250 - Avg Train Loss: 0.1566\n",
      "Batch 1100/1250 - Avg Train Loss: 0.1557\n",
      "Batch 1150/1250 - Avg Train Loss: 0.1570\n",
      "Batch 1200/1250 - Avg Train Loss: 0.1582\n",
      "Batch 1250/1250 - Avg Train Loss: 0.1584\n",
      "Epoch [28/30] Train Loss: 0.158446 | Val Loss: 0.666724 | LR: 0.000250\n",
      "\n",
      "===== Epoch 29/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.1747\n",
      "Batch 100/1250 - Avg Train Loss: 0.1548\n",
      "Batch 150/1250 - Avg Train Loss: 0.1504\n",
      "Batch 200/1250 - Avg Train Loss: 0.1450\n",
      "Batch 250/1250 - Avg Train Loss: 0.1466\n",
      "Batch 300/1250 - Avg Train Loss: 0.1467\n",
      "Batch 350/1250 - Avg Train Loss: 0.1449\n",
      "Batch 400/1250 - Avg Train Loss: 0.1473\n",
      "Batch 450/1250 - Avg Train Loss: 0.1493\n",
      "Batch 500/1250 - Avg Train Loss: 0.1469\n",
      "Batch 550/1250 - Avg Train Loss: 0.1482\n",
      "Batch 600/1250 - Avg Train Loss: 0.1486\n",
      "Batch 650/1250 - Avg Train Loss: 0.1485\n",
      "Batch 700/1250 - Avg Train Loss: 0.1506\n",
      "Batch 750/1250 - Avg Train Loss: 0.1504\n",
      "Batch 800/1250 - Avg Train Loss: 0.1515\n",
      "Batch 850/1250 - Avg Train Loss: 0.1517\n",
      "Batch 900/1250 - Avg Train Loss: 0.1519\n",
      "Batch 950/1250 - Avg Train Loss: 0.1527\n",
      "Batch 1000/1250 - Avg Train Loss: 0.1518\n",
      "Batch 1050/1250 - Avg Train Loss: 0.1527\n",
      "Batch 1100/1250 - Avg Train Loss: 0.1541\n",
      "Batch 1150/1250 - Avg Train Loss: 0.1529\n",
      "Batch 1200/1250 - Avg Train Loss: 0.1539\n",
      "Batch 1250/1250 - Avg Train Loss: 0.1537\n",
      "ðŸ”» LR reduced from 0.000250 to 0.000125\n",
      "Epoch [29/30] Train Loss: 0.153702 | Val Loss: 0.676012 | LR: 0.000125\n",
      "\n",
      "===== Epoch 30/30 =====\n",
      "âœ… Successfully fetched first training batch from DataLoader.\n",
      "Batch 50/1250 - Avg Train Loss: 0.1474\n",
      "Batch 100/1250 - Avg Train Loss: 0.1361\n",
      "Batch 150/1250 - Avg Train Loss: 0.1429\n",
      "Batch 200/1250 - Avg Train Loss: 0.1358\n",
      "Batch 250/1250 - Avg Train Loss: 0.1417\n",
      "Batch 300/1250 - Avg Train Loss: 0.1382\n",
      "Batch 350/1250 - Avg Train Loss: 0.1431\n",
      "Batch 400/1250 - Avg Train Loss: 0.1418\n",
      "Batch 450/1250 - Avg Train Loss: 0.1420\n",
      "Batch 500/1250 - Avg Train Loss: 0.1418\n",
      "Batch 550/1250 - Avg Train Loss: 0.1413\n",
      "Batch 600/1250 - Avg Train Loss: 0.1412\n",
      "Batch 650/1250 - Avg Train Loss: 0.1406\n",
      "Batch 700/1250 - Avg Train Loss: 0.1410\n",
      "Batch 750/1250 - Avg Train Loss: 0.1409\n",
      "Batch 800/1250 - Avg Train Loss: 0.1393\n",
      "Batch 850/1250 - Avg Train Loss: 0.1392\n",
      "Batch 900/1250 - Avg Train Loss: 0.1389\n",
      "Batch 950/1250 - Avg Train Loss: 0.1390\n",
      "Batch 1000/1250 - Avg Train Loss: 0.1389\n",
      "Batch 1050/1250 - Avg Train Loss: 0.1399\n",
      "Batch 1100/1250 - Avg Train Loss: 0.1391\n",
      "Batch 1150/1250 - Avg Train Loss: 0.1388\n",
      "Batch 1200/1250 - Avg Train Loss: 0.1383\n",
      "Batch 1250/1250 - Avg Train Loss: 0.1393\n",
      "Epoch [30/30] Train Loss: 0.139298 | Val Loss: 0.682233 | LR: 0.000125\n",
      "âœ… Training complete. Final model saved to 'results/ocr_model_final.pth'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5 (debugged) Train Model â€” safe & verbose (copy to replace your Block 5)\n",
    "# ============================================\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "EPOCHS = 30\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_path = \"results/best_ocr_model.pth\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    # ---------- TRAINING LOOP ----------\n",
    "    try:\n",
    "        # wrap iteration in try so we catch loader errors immediately\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch_count += 1\n",
    "\n",
    "            # Debug: show that we successfully retrieved a batch\n",
    "            if batch_idx == 0:\n",
    "                print(\"âœ… Successfully fetched first training batch from DataLoader.\")\n",
    "\n",
    "            # Unpack depending on collate format (keeps compatibility)\n",
    "            # Our collate_fn returns: imgs, targets, target_lengths, labels_padded, raw_texts\n",
    "            if len(batch) == 5:\n",
    "                imgs, targets, target_lengths, labels_padded, raw_texts = batch\n",
    "            elif len(batch) == 4:\n",
    "                imgs, targets, target_lengths, labels_padded = batch\n",
    "                raw_texts = [\"\"] * imgs.size(0)\n",
    "            elif len(batch) == 3:\n",
    "                imgs, targets, target_lengths = batch\n",
    "                labels_padded = None\n",
    "                raw_texts = [\"\"] * imgs.size(0)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected batch tuple length: {len(batch)}\")\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)              # (B, W, C)\n",
    "            outputs = outputs.permute(1, 0, 2) # (W, B, C) for CTCLoss\n",
    "\n",
    "            input_lengths = torch.full(\n",
    "                (outputs.size(1),),\n",
    "                outputs.size(0),\n",
    "                dtype=torch.long\n",
    "            ).to(device)\n",
    "\n",
    "            loss = ctc_loss_fn(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"âš ï¸ Skipping batch {batch_idx} due to NaN/Inf loss.\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # periodic print\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                avg_loss = running_train_loss / (batch_idx + 1)\n",
    "                print(f\"Batch {batch_idx+1}/{len(train_loader)} - Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # If DataLoader worker crashed or another error happened, print full stack and re-raise\n",
    "        import traceback\n",
    "        print(\"âŒ Exception while iterating train_loader:\")\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "    avg_train_loss = running_train_loss / max(1, batch_count)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # ---------- VALIDATION LOOP ----------\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                val_batch_count += 1\n",
    "                if len(batch) == 5:\n",
    "                    imgs, targets, target_lengths, labels_padded, raw_texts = batch\n",
    "                elif len(batch) == 4:\n",
    "                    imgs, targets, target_lengths, labels_padded = batch\n",
    "                    raw_texts = [\"\"] * imgs.size(0)\n",
    "                elif len(batch) == 3:\n",
    "                    imgs, targets, target_lengths = batch\n",
    "                    labels_padded = None\n",
    "                    raw_texts = [\"\"] * imgs.size(0)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                imgs = imgs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                target_lengths = target_lengths.to(device)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "                outputs = outputs.permute(1, 0, 2)\n",
    "                input_lengths = torch.full(\n",
    "                    (outputs.size(1),),\n",
    "                    outputs.size(0),\n",
    "                    dtype=torch.long\n",
    "                ).to(device)\n",
    "                loss = ctc_loss_fn(outputs, targets, input_lengths, target_lengths)\n",
    "                running_val_loss += loss.item()\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"âŒ Exception while iterating val_loader:\")\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "    avg_val_loss = running_val_loss / max(1, val_batch_count)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # scheduler & lr print\n",
    "    prev_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(avg_val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != prev_lr:\n",
    "        print(f\"ðŸ”» LR reduced from {prev_lr:.6f} to {new_lr:.6f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | LR: {new_lr:.6f}\")\n",
    "\n",
    "    # ---------- SAVE BEST ----------\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_loss\": best_val_loss\n",
    "        }, best_model_path)\n",
    "        print(f\"ðŸ’¾ Saved best model to {best_model_path}\")\n",
    "\n",
    "# final save\n",
    "torch.save(model.state_dict(), \"results/ocr_model_final.pth\")\n",
    "print(\"âœ… Training complete. Final model saved to 'results/ocr_model_final.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57d9711-145f-4137-88b7-544f38fb5e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 0 -> results/sample_pred_0_20251015_131450.png\n",
      "Saved sample 1 -> results/sample_pred_1_20251015_131450.png\n",
      "Saved sample 2 -> results/sample_pred_2_20251015_131451.png\n",
      "Saved sample 3 -> results/sample_pred_3_20251015_131451.png\n",
      "Saved sample 4 -> results/sample_pred_4_20251015_131451.png\n",
      "Saved sample 5 -> results/sample_pred_5_20251015_131451.png\n",
      "Saved sample 6 -> results/sample_pred_6_20251015_131451.png\n",
      "Saved sample 7 -> results/sample_pred_7_20251015_131451.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 6. Greedy decoder and sample predictions (robust)\n",
    "# ============================================\n",
    "def greedy_decoder(output, num_to_char):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoder: input output is (B, W, C) with log_softmax applied.\n",
    "    returns list of decoded strings (one per batch item)\n",
    "    \"\"\"\n",
    "    # output: (B, W, C)\n",
    "    preds = torch.argmax(output, 2).cpu().numpy()  # (B, W)\n",
    "    texts = []\n",
    "    for o in preds:\n",
    "        text = \"\"\n",
    "        prev = 0\n",
    "        for idx in o:\n",
    "            if idx != prev and idx != 0:\n",
    "                text += num_to_char.get(int(idx), \"\")\n",
    "            prev = idx\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# mapping consistent with datasets\n",
    "num_to_char = val_dataset.num_to_char\n",
    "char_to_num = val_dataset.char_to_num\n",
    "\n",
    "# run on one batch and print/save few samples\n",
    "model.eval()\n",
    "imgs, targets, target_lengths, labels_padded, raw_texts = next(iter(val_loader))\n",
    "imgs_cuda = imgs.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(imgs_cuda)  # (B, W, C)\n",
    "    decoded_texts = greedy_decoder(outputs, num_to_char)\n",
    "\n",
    "# Reconstruct true_texts from labels_padded and target_lengths (padded)\n",
    "true_texts = []\n",
    "for row_idx in range(labels_padded.size(0)):\n",
    "    length = int(target_lengths[row_idx])\n",
    "    row = labels_padded[row_idx][:length].cpu().numpy()\n",
    "    true_texts.append(\"\".join([num_to_char.get(int(x), \"\") for x in row]))\n",
    "\n",
    "# Save sample images with GT and Pred text\n",
    "for i in range(min(8, imgs.size(0))):\n",
    "    arr = imgs[i].cpu().squeeze().numpy()\n",
    "    plt.figure(figsize=(4,1.5))\n",
    "    plt.imshow(arr, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"GT: {true_texts[i]}\\nPred: {decoded_texts[i]}\")\n",
    "    fname = f\"results/sample_pred_{i}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    plt.savefig(fname, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved sample {i} -> {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25284e90-d1a8-4953-8544-f217c2cffb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Exact-match Accuracy: 0.6330\n",
      "Character-level Precision: 0.7612, Recall: 0.7355, F1: 0.7482\n",
      "Saved results/precision_recall_f1.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7. Compute character-level Precision / Recall / F1 and exact-match accuracy (and save bar graph)\n",
    "# ============================================\n",
    "def char_level_prf(preds, trues):\n",
    "    \"\"\"\n",
    "    Simple character-level TP/FP/FN counting using position alignment:\n",
    "      - For positions where both exist: if equal -> TP; else -> FP and FN (substitution)\n",
    "      - If predicted only -> FP\n",
    "      - If true only -> FN\n",
    "    returns precision, recall, f1\n",
    "    \"\"\"\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for p, t in zip(preds, trues):\n",
    "        lp = len(p)\n",
    "        lt = len(t)\n",
    "        L = max(lp, lt)\n",
    "        for i in range(L):\n",
    "            has_p = i < lp\n",
    "            has_t = i < lt\n",
    "            if has_p and has_t:\n",
    "                if p[i] == t[i]:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "                    FN += 1\n",
    "            elif has_p and (not has_t):\n",
    "                FP += 1\n",
    "            elif has_t and (not has_p):\n",
    "                FN += 1\n",
    "    eps = 1e-8\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    recall = TP / (TP + FN + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Build predictions and ground truths for entire validation set (may be slow if val set large)\n",
    "all_preds = []\n",
    "all_trues = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, targets, target_lengths, labels_padded, raw_texts in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs)  # (B,W,C)\n",
    "        batch_preds = greedy_decoder(outputs, num_to_char)\n",
    "        # reconstruct trues from labels_padded and lengths\n",
    "        for row_idx in range(labels_padded.size(0)):\n",
    "            length = int(target_lengths[row_idx])\n",
    "            row = labels_padded[row_idx][:length].cpu().numpy()\n",
    "            true_text = \"\".join([num_to_char.get(int(x), \"\") for x in row])\n",
    "            all_trues.append(true_text)\n",
    "        all_preds.extend(batch_preds)\n",
    "\n",
    "# compute exact match accuracy\n",
    "exact_matches = sum([1 for p,t in zip(all_preds, all_trues) if p == t])\n",
    "accuracy = exact_matches / max(1, len(all_trues))\n",
    "precision, recall, f1 = char_level_prf(all_preds, all_trues)\n",
    "print(f\"Validation Exact-match Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Character-level Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# save bar graph for precision, recall, f1\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([\"Precision\",\"Recall\",\"F1\"], [precision, recall, f1])\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Character-level Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.savefig(\"results/precision_recall_f1.png\", bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved results/precision_recall_f1.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a61eac9f-6f0d-410d-a6eb-11d41c0d62b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results/loss_curve.png\n",
      "Saved results/accuracy_point.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8. Save accuracy and loss graphs (and to disk)\n",
    "# ============================================\n",
    "# Loss curves (train & val)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"results/loss_curve.png\", bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved results/loss_curve.png\")\n",
    "\n",
    "# Accuracy (we only computed final val exact-match) -> create a simple line for demonstration\n",
    "# If you want per-epoch accuracy, you'd compute during training similarly to val loss aggregation per epoch.\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([accuracy], marker='o')  # single-point placeholder (replace with per-epoch values if computed)\n",
    "plt.title(\"Validation Exact-match Accuracy (final)\")\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(\"results/accuracy_point.png\", bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved results/accuracy_point.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c90226-d766-4441-ace2-20e30110450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 9. Save the final mapping and metadata\n",
    "# ============================================\n",
    "import json\n",
    "meta = {\n",
    "    \"characters\": characters,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"img_height\": IMG_HEIGHT,\n",
    "    \"img_width\": IMG_WIDTH,\n",
    "    \"seed\": SEED,\n",
    "    \"best_val_loss\": float(best_val_loss),\n",
    "    \"saved_at\": datetime.now().isoformat()\n",
    "}\n",
    "with open(\"results/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved results/metadata.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf9c077-1eba-4ea1-b256-d906a0f0866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Files saved into the results/ folder:\n",
      "  - accuracy_point.png\n",
      "  - best_ocr_model.pth\n",
      "  - loss_curve.png\n",
      "  - metadata.json\n",
      "  - ocr_model_final.pth\n",
      "  - precision_recall_f1.png\n",
      "  - sample_pred_0_20251015_131450.png\n",
      "  - sample_pred_1_20251015_131450.png\n",
      "  - sample_pred_2_20251015_131451.png\n",
      "  - sample_pred_3_20251015_131451.png\n",
      "  - sample_pred_4_20251015_131451.png\n",
      "  - sample_pred_5_20251015_131451.png\n",
      "  - sample_pred_6_20251015_131451.png\n",
      "  - sample_pred_7_20251015_131451.png\n",
      "\n",
      "Tips if performance still low:\n",
      " - Increase dataset size or use synthetic augmentation.\n",
      " - If sequences are longer than model capacity, increase IMG_WIDTH or reduce pooling.\n",
      " - Try a CRNN variant with deeper RNN or transformer-based sequence head if needed.\n",
      " - Consider using beam search decoder (with language model) for better final predictions.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 10. Notes & tips (print for convenience)\n",
    "# ============================================\n",
    "print(\"Done. Files saved into the results/ folder:\")\n",
    "for fname in sorted(os.listdir(\"results\")):\n",
    "    print(\"  -\", fname)\n",
    "print(\"\\nTips if performance still low:\")\n",
    "print(\" - Increase dataset size or use synthetic augmentation.\")\n",
    "print(\" - If sequences are longer than model capacity, increase IMG_WIDTH or reduce pooling.\")\n",
    "print(\" - Try a CRNN variant with deeper RNN or transformer-based sequence head if needed.\")\n",
    "print(\" - Consider using beam search decoder (with language model) for better final predictions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
